[
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "DagBag",
        "importPath": "airflow.models",
        "description": "airflow.models",
        "isExtraImport": true,
        "detail": "airflow.models",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "airflow.models",
        "description": "airflow.models",
        "isExtraImport": true,
        "detail": "airflow.models",
        "documentation": {}
    },
    {
        "label": "Connection",
        "importPath": "airflow.models",
        "description": "airflow.models",
        "isExtraImport": true,
        "detail": "airflow.models",
        "documentation": {}
    },
    {
        "label": "DagBag",
        "importPath": "airflow.models",
        "description": "airflow.models",
        "isExtraImport": true,
        "detail": "airflow.models",
        "documentation": {}
    },
    {
        "label": "BaseHook",
        "importPath": "airflow.hooks.base",
        "description": "airflow.hooks.base",
        "isExtraImport": true,
        "detail": "airflow.hooks.base",
        "documentation": {}
    },
    {
        "label": "initdb",
        "importPath": "airflow.utils.db",
        "description": "airflow.utils.db",
        "isExtraImport": true,
        "detail": "airflow.utils.db",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "kaggle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "kaggle",
        "description": "kaggle",
        "detail": "kaggle",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "snowflake.connector",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "snowflake.connector",
        "description": "snowflake.connector",
        "detail": "snowflake.connector",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "ProfileConfig",
        "importPath": "cosmos.config",
        "description": "cosmos.config",
        "isExtraImport": true,
        "detail": "cosmos.config",
        "documentation": {}
    },
    {
        "label": "ProjectConfig",
        "importPath": "cosmos.config",
        "description": "cosmos.config",
        "isExtraImport": true,
        "detail": "cosmos.config",
        "documentation": {}
    },
    {
        "label": "dag",
        "importPath": "airflow.decorators",
        "description": "airflow.decorators",
        "isExtraImport": true,
        "detail": "airflow.decorators",
        "documentation": {}
    },
    {
        "label": "task",
        "importPath": "airflow.decorators",
        "description": "airflow.decorators",
        "isExtraImport": true,
        "detail": "airflow.decorators",
        "documentation": {}
    },
    {
        "label": "EmailOperator",
        "importPath": "airflow.operators.email_operator",
        "description": "airflow.operators.email_operator",
        "isExtraImport": true,
        "detail": "airflow.operators.email_operator",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "magic_dict",
        "kind": 6,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "class magic_dict(dict):\n    def __init__(self, *args, **kwargs):\n        self.update(*args, **kwargs)\n    def __getitem__(self, key):\n        return {}.get(key, \"MOCKED_KEY_VALUE\")\n_no_default = object()  # allow falsey defaults\ndef variable_get_monkeypatch(key: str, default_var=_no_default, deserialize_json=False):\n    print(\n        f\"Attempted to get Variable value during parse, returning a mocked value for {key}\"\n    )",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "basehook_get_connection_monkeypatch",
        "kind": 2,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "def basehook_get_connection_monkeypatch(key: str, *args, **kwargs):\n    print(\n        f\"Attempted to fetch connection during parse returning an empty Connection object for {key}\"\n    )\n    return Connection(key)\nBaseHook.get_connection = basehook_get_connection_monkeypatch\n# # =========== /MONKEYPATCH BASEHOOK.GET_CONNECTION() ===========\n# =========== MONKEYPATCH OS.GETENV() ===========\ndef os_getenv_monkeypatch(key: str, *args, **kwargs):\n    default = None",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "os_getenv_monkeypatch",
        "kind": 2,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "def os_getenv_monkeypatch(key: str, *args, **kwargs):\n    default = None\n    if args:\n        default = args[0]  # os.getenv should get at most 1 arg after the key\n    if kwargs:\n        default = kwargs.get(\n            \"default\", None\n        )  # and sometimes kwarg if people are using the sig\n    env_value = os.environ.get(key, None)\n    if env_value:",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "variable_get_monkeypatch",
        "kind": 2,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "def variable_get_monkeypatch(key: str, default_var=_no_default, deserialize_json=False):\n    print(\n        f\"Attempted to get Variable value during parse, returning a mocked value for {key}\"\n    )\n    if default_var is not _no_default:\n        return default_var\n    if deserialize_json:\n        return magic_dict()\n    return \"NON_DEFAULT_MOCKED_VARIABLE_VALUE\"\nVariable.get = variable_get_monkeypatch",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "suppress_logging",
        "kind": 2,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "def suppress_logging(namespace):\n    \"\"\"\n    Suppress logging within a specific namespace to keep tests \"clean\" during build\n    \"\"\"\n    logger = logging.getLogger(namespace)\n    old_value = logger.disabled\n    logger.disabled = True\n    try:\n        yield\n    finally:",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "get_import_errors",
        "kind": 2,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "def get_import_errors():\n    \"\"\"\n    Generate a tuple for import errors in the dag bag, and include DAGs without errors.\n    \"\"\"\n    with suppress_logging(\"airflow\"):\n        dag_bag = DagBag(include_examples=False)\n        def strip_path_prefix(path):\n            return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n        # Initialize an empty list to store the tuples\n        result = []",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "test_file_imports",
        "kind": 2,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "def test_file_imports(rel_path, rv):\n    \"\"\"Test for import errors on a file\"\"\"\n    if os.path.exists(\".astro/dag_integrity_exceptions.txt\"):\n        with open(\".astro/dag_integrity_exceptions.txt\", \"r\") as f:\n            exceptions = f.readlines()\n    print(f\"Exceptions: {exceptions}\")\n    if (rv != \"No import errors\") and rel_path not in exceptions:\n        # If rv is not \"No import errors,\" consider it a failed test\n        raise Exception(f\"{rel_path} failed to import with message \\n {rv}\")\n    else:",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "BaseHook.get_connection",
        "kind": 5,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "BaseHook.get_connection = basehook_get_connection_monkeypatch\n# # =========== /MONKEYPATCH BASEHOOK.GET_CONNECTION() ===========\n# =========== MONKEYPATCH OS.GETENV() ===========\ndef os_getenv_monkeypatch(key: str, *args, **kwargs):\n    default = None\n    if args:\n        default = args[0]  # os.getenv should get at most 1 arg after the key\n    if kwargs:\n        default = kwargs.get(\n            \"default\", None",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "os.getenv",
        "kind": 5,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "os.getenv = os_getenv_monkeypatch\n# # =========== /MONKEYPATCH OS.GETENV() ===========\n# =========== MONKEYPATCH VARIABLE.GET() ===========\nclass magic_dict(dict):\n    def __init__(self, *args, **kwargs):\n        self.update(*args, **kwargs)\n    def __getitem__(self, key):\n        return {}.get(key, \"MOCKED_KEY_VALUE\")\n_no_default = object()  # allow falsey defaults\ndef variable_get_monkeypatch(key: str, default_var=_no_default, deserialize_json=False):",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "_no_default",
        "kind": 5,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "_no_default = object()  # allow falsey defaults\ndef variable_get_monkeypatch(key: str, default_var=_no_default, deserialize_json=False):\n    print(\n        f\"Attempted to get Variable value during parse, returning a mocked value for {key}\"\n    )\n    if default_var is not _no_default:\n        return default_var\n    if deserialize_json:\n        return magic_dict()\n    return \"NON_DEFAULT_MOCKED_VARIABLE_VALUE\"",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "Variable.get",
        "kind": 5,
        "importPath": ".astro.test_dag_integrity_default",
        "description": ".astro.test_dag_integrity_default",
        "peekOfCode": "Variable.get = variable_get_monkeypatch\n# # =========== /MONKEYPATCH VARIABLE.GET() ===========\n@contextmanager\ndef suppress_logging(namespace):\n    \"\"\"\n    Suppress logging within a specific namespace to keep tests \"clean\" during build\n    \"\"\"\n    logger = logging.getLogger(namespace)\n    old_value = logger.disabled\n    logger.disabled = True",
        "detail": ".astro.test_dag_integrity_default",
        "documentation": {}
    },
    {
        "label": "extract_from_kaggle",
        "kind": 2,
        "importPath": "dags.extract.get_data_kaggle",
        "description": "dags.extract.get_data_kaggle",
        "peekOfCode": "def extract_from_kaggle():\n    # Set directory to dump the datasets from kaggle\n    dataset_directory = Path(__file__).parent / 'datasets'\n    dataset_directory.mkdir(parents=True, exist_ok=True)\n    # Download dataset from Kaggle\n    kaggle.api.dataset_download_files(\"oleksiimartusiuk/e-commerce-data-shein\", path=dataset_directory, unzip=True)\n    # Load datasets into pandas DataFrames\n    csv_files = list(dataset_directory.glob(\"*.csv\"))\n    for file in csv_files:\n        try:",
        "detail": "dags.extract.get_data_kaggle",
        "documentation": {}
    },
    {
        "label": "dump_data",
        "kind": 2,
        "importPath": "dags.load.load_data_snowflake",
        "description": "dags.load.load_data_snowflake",
        "peekOfCode": "def dump_data():\n    env_path = Path(__file__).resolve().parent.parent.parent/'.env'\n    load_dotenv(dotenv_path=env_path)\n    schema_name = os.getenv('SNOWFLAKE_SCHEMA')\n    role = os.getenv('SNOWFLAKE_ROLE')\n    stage_creation_query = f\"\"\"\n    CREATE STAGE IF NOT EXISTS {schema_name}.product_stage\n    FILE_FORMAT=(TYPE=PARQUET);\n    \"\"\"\n    # Connect to Snowflake",
        "detail": "dags.load.load_data_snowflake",
        "documentation": {}
    },
    {
        "label": "DBT_CONFIG",
        "kind": 5,
        "importPath": "dags.transform.cosmos_config",
        "description": "dags.transform.cosmos_config",
        "peekOfCode": "DBT_CONFIG = ProfileConfig(\n    profile_name='e-commerce',\n    target_name='dev',\n    profiles_yml_filepath=Path('/usr/local/airflow/dags/transform/profiles.yml')\n)\nDBT_PROJECT_CONFIG = ProjectConfig(\n    dbt_project_path='/usr/local/airflow/dags/transform/',\n)",
        "detail": "dags.transform.cosmos_config",
        "documentation": {}
    },
    {
        "label": "DBT_PROJECT_CONFIG",
        "kind": 5,
        "importPath": "dags.transform.cosmos_config",
        "description": "dags.transform.cosmos_config",
        "peekOfCode": "DBT_PROJECT_CONFIG = ProjectConfig(\n    dbt_project_path='/usr/local/airflow/dags/transform/',\n)",
        "detail": "dags.transform.cosmos_config",
        "documentation": {}
    },
    {
        "label": "ecommerce_data_pipeline",
        "kind": 2,
        "importPath": "dags.dag",
        "description": "dags.dag",
        "peekOfCode": "def ecommerce_data_pipeline():\n    from extract.get_data_kaggle import extract_from_kaggle\n    @task()\n    def extract():\n        \"\"\"This function will extract data from Kaggle and convert it from csv to parquet.\"\"\"\n        extract_from_kaggle()  \n    @task()\n    def load():\n        from load.load_data_snowflake import dump_data\n        \"\"\"This function will load the parquet data into Snowflake.\"\"\"",
        "detail": "dags.dag",
        "documentation": {}
    },
    {
        "label": "default_args",
        "kind": 5,
        "importPath": "dags.dag",
        "description": "dags.dag",
        "peekOfCode": "default_args = {\n    \"owner\": \"Melvin\",\n    \"start_date\": datetime(2025, 2, 1),\n    \"execution_timeout\": timedelta(hours=2),\n    \"email_on_failure\": True,\n    \"email_on_retry\": True,\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=1),\n    \"email_subject\": \"E-Commerce Data Pipeline\",\n    \"email_recipients\": os.getenv('EMAIL_RECIPIENT')  # Replace with your email address",
        "detail": "dags.dag",
        "documentation": {}
    },
    {
        "label": "ecommerce_data_pipeline_dag",
        "kind": 5,
        "importPath": "dags.dag",
        "description": "dags.dag",
        "peekOfCode": "ecommerce_data_pipeline_dag = ecommerce_data_pipeline()",
        "detail": "dags.dag",
        "documentation": {}
    },
    {
        "label": "suppress_logging",
        "kind": 2,
        "importPath": "tests.dags.test_dag_example",
        "description": "tests.dags.test_dag_example",
        "peekOfCode": "def suppress_logging(namespace):\n    logger = logging.getLogger(namespace)\n    old_value = logger.disabled\n    logger.disabled = True\n    try:\n        yield\n    finally:\n        logger.disabled = old_value\ndef get_import_errors():\n    \"\"\"",
        "detail": "tests.dags.test_dag_example",
        "documentation": {}
    },
    {
        "label": "get_import_errors",
        "kind": 2,
        "importPath": "tests.dags.test_dag_example",
        "description": "tests.dags.test_dag_example",
        "peekOfCode": "def get_import_errors():\n    \"\"\"\n    Generate a tuple for import errors in the dag bag\n    \"\"\"\n    with suppress_logging(\"airflow\"):\n        dag_bag = DagBag(include_examples=False)\n        def strip_path_prefix(path):\n            return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n        # prepend \"(None,None)\" to ensure that a test object is always created even if it's a no op.\n        return [(None, None)] + [",
        "detail": "tests.dags.test_dag_example",
        "documentation": {}
    },
    {
        "label": "get_dags",
        "kind": 2,
        "importPath": "tests.dags.test_dag_example",
        "description": "tests.dags.test_dag_example",
        "peekOfCode": "def get_dags():\n    \"\"\"\n    Generate a tuple of dag_id, <DAG objects> in the DagBag\n    \"\"\"\n    with suppress_logging(\"airflow\"):\n        dag_bag = DagBag(include_examples=False)\n    def strip_path_prefix(path):\n        return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n    return [(k, v, strip_path_prefix(v.fileloc)) for k, v in dag_bag.dags.items()]\n@pytest.mark.parametrize(",
        "detail": "tests.dags.test_dag_example",
        "documentation": {}
    },
    {
        "label": "test_file_imports",
        "kind": 2,
        "importPath": "tests.dags.test_dag_example",
        "description": "tests.dags.test_dag_example",
        "peekOfCode": "def test_file_imports(rel_path, rv):\n    \"\"\"Test for import errors on a file\"\"\"\n    if rel_path and rv:\n        raise Exception(f\"{rel_path} failed to import with message \\n {rv}\")\nAPPROVED_TAGS = {}\n@pytest.mark.parametrize(\n    \"dag_id,dag,fileloc\", get_dags(), ids=[x[2] for x in get_dags()]\n)\ndef test_dag_tags(dag_id, dag, fileloc):\n    \"\"\"",
        "detail": "tests.dags.test_dag_example",
        "documentation": {}
    },
    {
        "label": "test_dag_tags",
        "kind": 2,
        "importPath": "tests.dags.test_dag_example",
        "description": "tests.dags.test_dag_example",
        "peekOfCode": "def test_dag_tags(dag_id, dag, fileloc):\n    \"\"\"\n    test if a DAG is tagged and if those TAGs are in the approved list\n    \"\"\"\n    assert dag.tags, f\"{dag_id} in {fileloc} has no tags\"\n    if APPROVED_TAGS:\n        assert not set(dag.tags) - APPROVED_TAGS\n@pytest.mark.parametrize(\n    \"dag_id,dag, fileloc\", get_dags(), ids=[x[2] for x in get_dags()]\n)",
        "detail": "tests.dags.test_dag_example",
        "documentation": {}
    },
    {
        "label": "test_dag_retries",
        "kind": 2,
        "importPath": "tests.dags.test_dag_example",
        "description": "tests.dags.test_dag_example",
        "peekOfCode": "def test_dag_retries(dag_id, dag, fileloc):\n    \"\"\"\n    test if a DAG has retries set\n    \"\"\"\n    assert (\n        dag.default_args.get(\"retries\", None) >= 2\n    ), f\"{dag_id} in {fileloc} must have task retries >= 2.\"",
        "detail": "tests.dags.test_dag_example",
        "documentation": {}
    },
    {
        "label": "APPROVED_TAGS",
        "kind": 5,
        "importPath": "tests.dags.test_dag_example",
        "description": "tests.dags.test_dag_example",
        "peekOfCode": "APPROVED_TAGS = {}\n@pytest.mark.parametrize(\n    \"dag_id,dag,fileloc\", get_dags(), ids=[x[2] for x in get_dags()]\n)\ndef test_dag_tags(dag_id, dag, fileloc):\n    \"\"\"\n    test if a DAG is tagged and if those TAGs are in the approved list\n    \"\"\"\n    assert dag.tags, f\"{dag_id} in {fileloc} has no tags\"\n    if APPROVED_TAGS:",
        "detail": "tests.dags.test_dag_example",
        "documentation": {}
    }
]